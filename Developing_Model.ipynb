{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Developing_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/SwapnilSParkhe/keras-flask-deploy-webapp/blob/master/Developing_Model.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vKHosnAe9jk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Developing Model:\n",
        "\n",
        " - Building Model Data (ADS) and Architecture \n",
        " - Fitting built Model to Data\n",
        " - Evaluating Model Performance"
      ]
    },
    {
      "metadata": {
        "id": "qD71eYV--JpO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Checking GPU status**"
      ]
    },
    {
      "metadata": {
        "id": "kea9R2Q--Sxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "3a531574-f0d8-446b-aa06-6c3ba8980e82"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 5257096417000077520, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11288900404\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 7908338663757240352\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "sx-pKMxI9jk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Analytical Dataset (ADS) Creation"
      ]
    },
    {
      "metadata": {
        "id": "6IOEkIvT9jk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Image ID or Identifiers**"
      ]
    },
    {
      "metadata": {
        "id": "HCPKCkuW_D0k",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2cc7a80e-7670-408d-f1d2-c266f8878dd3"
      },
      "cell_type": "code",
      "source": [
        "#Uploading relevant files from local to cloud (using google.colab lib)\n",
        "\n",
        "#Library for Uploading data from local to cloud\n",
        "from google.colab import files\n",
        "\n",
        "#Upload train image text\n",
        "files.upload()   #upload files \n",
        "\n",
        "#Upload valid image text\n",
        "files.upload()   #upload files\n",
        "\n",
        "#Upload test image text\n",
        "files.upload()   #upload files"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5eb75be5-b633-4661-827d-a1cc9cdf4d77\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5eb75be5-b633-4661-827d-a1cc9cdf4d77\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e056317-9abf-4a4f-ae91-b9e038b7fe96\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3e056317-9abf-4a4f-ae91-b9e038b7fe96\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b23464c-9ebb-4c3b-80d5-110eeb17b225\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8b23464c-9ebb-4c3b-80d5-110eeb17b225\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "g8KSTWjR9jk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing file: reading the content into Py file\n",
        "def import_file(input_file):\n",
        "    file=open(input_file,'r')   #creating a bridge btwn OS and Py files\n",
        "    content=file.read()   #reading content via the bridge\n",
        "    file.close()   #closing the bridge\n",
        "    return content\n",
        "\n",
        "imported_train=import_file('Flickr_8k.trainImages.txt') \n",
        "imported_valid=import_file('Flickr_8k.devImages.txt') \n",
        "imported_test=import_file('Flickr_8k.testImages.txt')    \n",
        "\n",
        "#Creating a set of image-IDs\n",
        "def create_img_set(file):\n",
        "    imgID_set=list()\n",
        "    for item in file.split('\\n'):   #accessing line by line\n",
        "        if len(item)<1:   #rejecting empty spaces\n",
        "            continue\n",
        "        imgID=item.split('.')[0]   #only taking imgID (rejecting 'jpg')\n",
        "        imgID_set.append(imgID)   #appending imgIDs to imgID_set\n",
        "    return set(imgID_set)\n",
        "\n",
        "imgID_trainset=create_img_set(imported_train)\n",
        "imgID_validset=create_img_set(imported_valid)\n",
        "imgID_testset=create_img_set(imported_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-KVDSdl9jlC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing previously created files (from PreprocessingData NoteBook): Img desc and Img features**"
      ]
    },
    {
      "metadata": {
        "id": "PjiYF0TQARnm",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9bc08b7d-35d5-4735-9082-25f1bcc39af5"
      },
      "cell_type": "code",
      "source": [
        "#Uploading relevant files from local to cloud (using google.colab lib)\n",
        "#Note: Preprocessing Desc and Feat was done solely on training data files\n",
        "\n",
        "#Library for Uploading data from local to cloud\n",
        "from google.colab import files\n",
        "\n",
        "#Upload cleaned organised text file (from Text precprosssing step)\n",
        "files.upload()   #upload files \n",
        "\n",
        "#Upload features file (from Image preprocessing step)\n",
        "files.upload()   #upload files"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec80c542-d74a-4aaa-b89c-4ba2b6c48fb6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ec80c542-d74a-4aaa-b89c-4ba2b6c48fb6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving cln_orgnse_text.txt to cln_orgnse_text.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ac1a69b8-6560-4f22-95de-a871fa9cc411\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ac1a69b8-6560-4f22-95de-a871fa9cc411\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.pkl to features.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9h3hx2SV9jlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing image desc files for this image data germane to training set\n",
        "def import_prepro_desc(prepro_file, dataset):\n",
        "    file=import_file(prepro_file)\n",
        "    desc=dict()\n",
        "    for item in file.split('\\n'):\n",
        "        tokens=item.split()   #splitting by whitespaces\n",
        "        image_ID,image_desc=tokens[0],tokens[1:]   #separating ID, desc\n",
        "        if image_ID in dataset:   #inner join imgID & training imgID \n",
        "            if image_ID not in desc:   #new list for new image_ID key \n",
        "                desc[image_ID]=list()\n",
        "            desc_='startseq ' + ' '.join(image_desc)+' endseq' # tokens\n",
        "            desc[image_ID].append(desc_)\n",
        "    return desc\n",
        "\n",
        "desc_train=import_prepro_desc('cln_orgnse_text.txt',imgID_trainset)\n",
        "desc_valid=import_prepro_desc('cln_orgnse_text.txt',imgID_validset)\n",
        "desc_test=import_prepro_desc('cln_orgnse_text.txt',imgID_testset)\n",
        "\n",
        "#Importing image features for this image data germane to training set\n",
        "from pickle import load\n",
        "def import_features(feature_file, dataset):\n",
        "    all_features = load(open(feature_file, 'rb'))  #load all features\n",
        "    features = {k: all_features[k] for k in dataset} #inner join\n",
        "    return features\n",
        "\n",
        "feature_train=import_features('features.pkl',imgID_trainset) #used later\n",
        "feature_valid=import_features('features.pkl',imgID_validset) #used later\n",
        "feature_test=import_features('features.pkl',imgID_testset) #used later"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qVVJsEMB9jlF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training data manipulations: Creating a custom Tokeizer function: Tokenizing descriptions**"
      ]
    },
    {
      "metadata": {
        "id": "xSizh-do9jlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "de139cc7-c457-4d64-ec45-4880872d4b37"
      },
      "cell_type": "code",
      "source": [
        "#Creating a simple list of desc from dict of desc\n",
        "def dict2list(input_dict):\n",
        "    desc_list=list()\n",
        "    for key in input_dict.keys():\n",
        "        [desc_list.append(d) for d in input_dict[key]]\n",
        "    return desc_list\n",
        "\n",
        "desc_train_list=dict2list(desc_train)\n",
        "\n",
        "#tokeinizing (could be improved by filetring english stopwords later)\n",
        "#Note: turning each text into sequence of integers (integer: token ID)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from pickle import dump\n",
        "def tokenize(input_list):\n",
        "    tokenizer=Tokenizer()\n",
        "    tokenizer.fit_on_texts(input_list)\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer=tokenize(desc_train_list) #to be used later\n",
        "vocab_size=len(tokenizer.word_index)+1 #to be used later\n",
        "print(\"Vocab Size:\",vocab_size)\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb')) #to be used later\n",
        "from google.colab import files\n",
        "files.download('tokenizer.pkl') #Download Tokenizer (used later)\n",
        "\n",
        "#Length of the description with the most words\n",
        "def max_length(desc_list):\n",
        "    max_len=max([len(item.split()) for item in desc_list])\n",
        "    return max_len\n",
        "max_length = max_length(desc_train_list) #to be used later\n",
        "print('Description Length', max_length)\n",
        "\n",
        "#Longest desc check\n",
        "def longest_desc(desc_list):\n",
        "    max_length=max([len(item.split()) for item in desc_list])\n",
        "    print(\"Max_len:\",max_length)\n",
        "    print(\"Desc:\", [item for item in desc_list if len(item.split())==max_length])\n",
        "\n",
        "longest_desc(desc_train_list)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 7266\n",
            "Description Length 33\n",
            "Max_len: 33\n",
            "Desc: ['startseq an man wearing green sweatshirt and blue vest is holding up dollar bills in front of his face while standing on busy sidewalk in front of group of men playing instruments endseq']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IvQh6N5V9jlN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**LSTM's Analytical Dataset: Input(ImageID and Seq_item)-Ouput(SeqWord) data**"
      ]
    },
    {
      "metadata": {
        "id": "RHUAPoyW9jlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating ADS for LSTM: Input(Image_ID and Seq_item)-Ouput(SeqWord)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "def create_ADS(tokenizer, max_length, desc_list, img):\n",
        "    X_img_ID, X_desc_item, y=list(), list(), list()\n",
        "    for desc in desc_list:\n",
        "        seq=tokenizer.texts_to_sequences([desc])[0] #encoding seq\n",
        "        for i in range(1,len(seq)):#split seq into multi X,y pairs\n",
        "            in_seq, out_seq=seq[:i], seq[i] #desc input-output pair\n",
        "            in_seq=pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq=to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            X_img_ID.append(img) #appending  img IDs\n",
        "            X_desc_item.append(in_seq)  #multi X-y pairs encoding\n",
        "            y.append(out_seq)   #oneHot encoded version of output word\n",
        "    return np.array(X_img_ID), np.array(X_desc_item), np.array(y)\n",
        "\n",
        "#Progressive Data Loading: Generate data (yield one photo’s data/batch) \n",
        "#Note: intended to be used in a call to model.fit_generator()\n",
        "def generate_data(tokenizer, max_length, desc_dict, img):\n",
        "    while 1:   #loop for ever over images\n",
        "        for key, desc_list in desc_dict.items(): #access image feature\n",
        "            img_ = img[key][0]  #image ID\n",
        "            in_img,in_seq,out_word=create_ADS(tokenizer,\n",
        "                                              max_length,\n",
        "                                              desc_list,img_)\n",
        "            yield [[in_img, in_seq], out_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJmJ8lWb9jlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the Model Architecture (Merge Model of Embeddings+LSTMs with CNN penultimate layer)\n",
        "**Note:** Combines both the encoded form (features) of the image input with the encoded form (context) of the text description generated so far; Combination of these two encoded inputs is then used by a very simple decoder model to generate the next word in the sequence"
      ]
    },
    {
      "metadata": {
        "id": "hCoFn7219jlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Merge Model of Image Captioning](https://i.pinimg.com/originals/35/8b/dc/358bdc11e71f8c78632560c7c819919d.png)"
      ]
    },
    {
      "metadata": {
        "id": "Cg8uJDkn9jlS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing relevant libraries**"
      ]
    },
    {
      "metadata": {
        "id": "flMCXHs69jlS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dropout, Dense #feat. encoding\n",
        "from keras.layers import Embedding, Dropout, LSTM #desc. encoding\n",
        "from keras.layers.merge import add #decoding\n",
        "from keras.models import Model #Model-Input-Output architecture"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVR6G_jU9jlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "15550d4c-84d5-4ac6-94db-34cda5d33ecf"
      },
      "cell_type": "code",
      "source": [
        "def build_model_arch(vocab_size, max_length):\n",
        "    #Encoder Models (Img-Feat and Desc Encoding)\n",
        "    #1.Image feature extractor model\n",
        "    feat_input=Input(shape=(4096,))\n",
        "    feat_1=Dropout(0.5)(feat_input)\n",
        "    feat_2=Dense(300, activation='relu')(feat_1)\n",
        "\n",
        "    #2.Embedding+LSTM sequence model\n",
        "    desc_input=Input(shape=(max_length,))\n",
        "    desc_1=Embedding(vocab_size, 300, mask_zero=True)(desc_input)\n",
        "    desc_2=Dropout(0.5)(desc_1)\n",
        "    desc_3=LSTM(300)(desc_2)\n",
        "\n",
        "    #Decoder Model ('adding' above encoding model layers; with FFNs)\n",
        "    deco_1=add([feat_2, desc_3]) #adding element wise for both vectors\n",
        "    deco_2=Dense(300, activation='relu')(deco_1)\n",
        "    output=Dense(vocab_size, activation='softmax')(deco_2)\n",
        "\n",
        "    #Creating Model-Input-Output architecture; Compiling (with loss, opt.)\n",
        "    model=Model(inputs=[feat_input, desc_input], outputs=output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    #Summarizing and Plotting model\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "model = build_model_arch(vocab_size, max_length)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 33)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 33, 300)      2179800     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 33, 300)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 300)          1229100     dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 300)          721200      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 300)          0           dense_4[0][0]                    \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 300)          90300       add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 7266)         2187066     dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 6,407,466\n",
            "Trainable params: 6,407,466\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N86jVNzm9jla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fitting Model to train data (validation data to balance bias-variance)"
      ]
    },
    {
      "metadata": {
        "id": "0AkfJjHm9jlc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining checkpoint callback; specifying model hyperparams\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = 'best_model_weights.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min')\n",
        "epochs_=10\n",
        "steps_train=len(desc_train)  #steps=N/batch_size\n",
        "steps_valid=len(desc_valid)  #steps=N/batch_size\n",
        "\n",
        "#Fitting model to generated data (along side validation loss checks)\n",
        "generated_data_train=generate_data(tokenizer, max_length, \n",
        "                                   desc_train, feature_train)\n",
        "generated_data_valid=generate_data(tokenizer, max_length, \n",
        "                                   desc_valid, feature_valid)\n",
        "model.fit_generator(generated_data_train, epochs=epochs_,\n",
        "                    steps_per_epoch=steps_train,\n",
        "                    validation_data=generated_data_valid,\n",
        "                    validation_steps=steps_valid,\n",
        "                    callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IXfGB-lGpgn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Downloading Best model (weights)\n",
        "from google.colab import files\n",
        "files.download('best_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xrdGHY2wVDaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#Training manually (if above code hangs or get stuck)\n",
        "epochs=20\n",
        "steps=len(desc_train)  #steps=N/batch_size\n",
        "for epoch in range(epochs):\n",
        "    generated_data_train=generate_data(tokenizer, max_length, \n",
        "                                       desc_train, feature_train)\n",
        "    generated_data_valid=generate_data(tokenizer, max_length, \n",
        "                                       desc_valid, feature_valid)\n",
        "    model.fit_generator(generated_data_train,steps_per_epoch=steps, \n",
        "                        validation_data= generated_data_valid,\n",
        "                        epochs=1, verbose=1)\n",
        "    model.save('model_' + str(epoch) + '.h5')\n",
        "####################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEr1X_x-oI5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model (on validaiton data using BLEU score)"
      ]
    },
    {
      "metadata": {
        "id": "ZLV4inNtobmG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Generating description for a photo using the trained model (for a given tokenizer on train data)**\n",
        "\n",
        "Note: passing in the start description token ‘start_seq‘, generating one word, then calling the model recursively with generated words as input until the end of sequence token is reached ‘end_seq‘ or the maximum description length is reached"
      ]
    },
    {
      "metadata": {
        "id": "Txv_pUQcoICb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Mapping an integer prediction back to a word\n",
        "#Note: Using the same tokeniser used for train data\n",
        "def intID_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index==integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "#Generating a desc for an image using trained model\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "def generate_desc(model, tokenizer, image, max_length):\n",
        "    in_text='startseq'   #seeding the generation process\n",
        "    for i in range(max_length):\n",
        "        seq=tokenizer.texts_to_sequences([in_text])[0] #encoding txt2int\n",
        "        seq=pad_sequences([seq], maxlen=max_length) #padding seq\n",
        "        pred=model.predict([image,seq], verbose=0)  #predict using model\n",
        "        pred=argmax(pred)   #prob to integer ID conversion\n",
        "        word=intID_to_word(pred, tokenizer) #intID to word mapping\n",
        "        if word is None:\n",
        "            break   #stop if cant map word\n",
        "        in_text += ' ' + word  #append as input to generate next word\n",
        "        if word=='endseq':\n",
        "            break   #stop if end of seq\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnmtWPnXojwE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Evaluating model**\n",
        "\n",
        "Note: for a given set of photo desc and photo features (on the validation data, could use test data later)"
      ]
    },
    {
      "metadata": {
        "id": "8ZTzXkT2qp0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d97b29f0-6bb9-4efa-f1f9-76b71808e9b2"
      },
      "cell_type": "code",
      "source": [
        "#Importing the nltk librarie's BLEU score evaluator\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "#Function for Evaluating model\n",
        "def evaluate_model(model, desc_dict, image, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    for key, desc_list in desc_dict.items():\n",
        "        pred_=generate_desc(model, tokenizer, image[key], max_length)\n",
        "        act_=[desc.split() for desc in desc_list]\n",
        "        #print(pred_,'\\n',act_,'\\n') #checking what's getting generated\n",
        "        predicted.append(pred_.split())\n",
        "        actual.append(act_)\n",
        "    #calculating BLEU score\n",
        "    print('BLEU-1:%f' % corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))\n",
        "    print('BLEU-2:%f' % corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))\n",
        "    print('BLEU-3:%f' % corpus_bleu(actual,predicted,weights=(0.3,0.3,0.3,0)))\n",
        "    print('BLEU-4:%f' % corpus_bleu(actual,predicted,\n",
        "                                    weights=(0.25,0.25,0.25,0.25)))\n",
        "    \n",
        "#Uploading, Loading and using best saved model\n",
        "from google.colab import files\n",
        "from keras.models import load_model\n",
        "  #files.upload()\n",
        "best_model = load_model('best_model_weights.h5')\n",
        "\n",
        "#Checking Performance on both valid and test data\n",
        "print('For valid data:', evaluate_model(best_model,desc_valid,feature_valid,tokenizer,max_length))\n",
        "print('\\n')\n",
        "print('For test data:', evaluate_model(best_model,desc_test,feature_test,tokenizer,max_length))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1:0.552174\n",
            "BLEU-2:0.298463\n",
            "BLEU-3:0.206645\n",
            "BLEU-4:0.099745\n",
            "For valid data: None\n",
            "\n",
            "\n",
            "BLEU-1:0.558602\n",
            "BLEU-2:0.306019\n",
            "BLEU-3:0.213852\n",
            "BLEU-4:0.104576\n",
            "For test data: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A3NINq0imv9V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WDLQ29DFmwVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXkYaBDtfA5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "diBCyJhl9kKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uyj39Meje7gq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}